{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed34b8eb-3179-4be8-92ba-7bddf9777473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spend Categorization - Naive LLM Calls\n",
    "This notebook uses a naive version of our to do the first two categories using only prompt engineering. It provides a baseline of using naive batch inference to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf4256e-0b85-4684-b58a-5a98242acb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "awards = spark.sql(\"SELECT * FROM shm.spend.awards_text\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc3d72c-6d72-462f-b987-627852fce997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "awards.iloc[0].combined_award_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547ab982-a6e9-45a5-8db9-cd37c0e684a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "awards.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498d1a49-270a-494e-8a4e-b3991fe1d114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"shm.spend.awards\").select(\"funding_agency_name\", \"funding_sub_agency_name\").distinct().toPandas()\n",
    "\n",
    "hierarchy = \"# Agency Hierarchy\\n\\n\"\n",
    "for agency in sorted(df['funding_agency_name'].dropna().unique()):\n",
    "    hierarchy += f\"## {agency}\\n\"\n",
    "    subagencies = df[df['funding_agency_name'] == agency]['funding_sub_agency_name'].dropna().unique()\n",
    "    for subagency in sorted(subagencies):\n",
    "        hierarchy += f\"- {subagency}\\n\"\n",
    "    hierarchy += \"\\n\"\n",
    "\n",
    "with open(\"agency_hierarchy.md\", \"w\") as f:\n",
    "    f.write(hierarchy)\n",
    "\n",
    "hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0118f60-059a-48b0-ac4e-932b5f71f1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell creates a markdown string with all the values from the agency and its subagencies. This is convenient because it always uses the awards table so is always up to date. It could also be pointed at a category tree for hierachical spend clasification.\n",
    "\n",
    "This could also be a function used as an agent tool.\n",
    "\n",
    "Next, we write a short prompt for our model - this could definitely be improved, but nowhere near enough to get acceptable accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f87f9ce-ca23-40d8-b8d8-d16727e690f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Use the following agency hierarchy and return the agency and sub agency for the award below. Return a json output with only the agencies. You must use the agencies and subagencies from the hierarchy, pick the best ones.\n",
    "\n",
    "For example:\n",
    "'# Contract CONT_IDV_05GA0A17A0017_0559\\n*date: 2017-03-30 00:00:00\\n*obligation: 0.0\\n*total value: 2500000.0\\n*recipient: SIGNET TECHNOLOGIES INCORPORATED\\n*location: BELTSVILLE, MARYLAND\\n*transaction description: IGF::CT::IGF  PROVIDE PREVENTIVE, NORMAL, AND EMERGENCY MAINTENANCE ON ALL COMPONENTS OF THE INTEGRATED ELECTRONIC SECURITY SYSTEM (IESS) AT THE GAO HEADQUARTERS AND 11 FIELD OFFICES.  IN ADDITION TO THE MAINTENANCE, THE CONTRACTOR WILL PROVIDE SUPPORT ON AN AS NEEDED BASIS FOR THE INSTALLATION AND/OR UPGRADE OF IESS COMPONENTS.\\n*product description: MAINT/REPAIR/REBUILD OF EQUIPMENT- ALARM, SIGNAL, AND SECURITY DETECTION SYSTEMS\\n*naics description: AUTOMATIC ENVIRONMENTAL CONTROL MANUFACTURING FOR RESIDENTIAL, COMMERCIAL, AND APPLIANCE USE'\n",
    "\n",
    "{'agency': Government Accountability Office, 'subagency': GAO, Except Comptroller General}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f3faca-7915-4378-9ebd-4bbde0504ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We setup widgets so that we can call the category tree and prompt in our batch inference as parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29ab9f4-95d3-4671-acb5-d21e6f25ae28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use AI_QUERY to run batch inference - this is all done in SQL - I am repeating the `CONCAT` call twice here, just so I can inspect the combined prompt that went into the model. We also use the `responseFormat` to enforce structured outputs. This is critical for consistency and maintanability of Generative AI solutions - I wouldn't leave POC without it. It's worth pointing out that because of the optimizations done in AI_QUERY - 500 calls to the LLM only takes 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740ddfa4-26da-4202-8bc7-68192c8d9e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "pred_naive_df = (\n",
    "    spark.table(\"shm.spend.test\")\n",
    "    .withColumn(\n",
    "        \"full_prompt\",\n",
    "        F.concat(\n",
    "            F.lit(prompt), \n",
    "            F.lit('\\n'),\n",
    "            F.lit(hierarchy), \n",
    "            F.lit('\\n'),\n",
    "            F.col(\"combined_award_text\")\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id\"),\n",
    "        F.col(\"full_prompt\").alias(\"prompt\"),\n",
    "        F.expr(\"\"\"\n",
    "            AI_QUERY(\n",
    "                'databricks-meta-llama-3-3-70b-instruct',\n",
    "                full_prompt,\n",
    "                responseFormat => '{\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": {\n",
    "                        \"name\": \"categorization\",\n",
    "                        \"schema\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"agency\": {\"type\": \"string\"},\n",
    "                                \"subagency\": {\"type\": \"string\"}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }'\n",
    "            )\n",
    "        \"\"\").alias(\"llm_output\")\n",
    "    )\n",
    ")\n",
    "\n",
    "pred_naive_df.write.mode(\"overwrite\").saveAsTable(\"shm.spend.pred_naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7540e7c3-8a27-4eba-95c7-55f49911df4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Second step overwrite to deconstruct that JSON file and pull in the actual labels for evaluation. This could be done in the first SQL call, but it was getting long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7573ccf1-da7f-4c9a-a706-0c3ce1883298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM shm.spend.pred_naive LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2784865-a366-4b4c-a59b-829c1d10c3d1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764305270784}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE shm.spend.pred_naive_comp AS\n",
    "SELECT\n",
    "  p.*,\n",
    "  agency,\n",
    "  subagency,\n",
    "  t.funding_agency_name,\n",
    "  t.funding_sub_agency_name\n",
    "FROM \n",
    "  shm.spend.pred_naive p\n",
    "JOIN\n",
    "  shm.spend.test t\n",
    "ON \n",
    "  t.id = p.id\n",
    "LATERAL VIEW \n",
    "  JSON_TUPLE(p.llm_output, 'agency', 'subagency') AS agency, subagency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc21622e-908a-460b-97c1-58fca9d9b836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's move into sklearn to get a classification report from our LLM based analysis for comparison sakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457cd0ae-dc8f-457d-b05b-5dfcbb9ecb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_naive = spark.table('shm.spend.pred_naive_comp').dropna(\n",
    "    subset=['funding_agency_name', 'agency', 'funding_sub_agency_name', 'subagency']\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce9ef25-c731-4ada-a60b-6a43acbc542a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(f\"\"\"Agency Accuracy: {accuracy_score(\n",
    "  pred_naive['funding_agency_name'], \n",
    "  pred_naive['agency']\n",
    "  ):0.3f}\"\"\")\n",
    "\n",
    "print(f\"\"\"Subagency Accuracy: {accuracy_score(\n",
    "  pred_naive['funding_sub_agency_name'], \n",
    "  pred_naive['subagency']\n",
    "  ):0.3f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cfaa24-4a7d-4081-be32-38f2f246d428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With naive inference we have relatively poor accuracy, but more than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1eda5e7-30da-456f-9852-42bcbc2b775b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM shm.spend.test"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6333049142028493,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1_batch_inference_naive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
