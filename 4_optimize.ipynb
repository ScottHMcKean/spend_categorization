{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed34b8eb-3179-4be8-92ba-7bddf9777473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spend Categorization\n",
    "\n",
    "## Prompt Optimizer and Evaluation\n",
    "The batch inference above works at scale, but the prompt engineering is a major challenge - we have a large hierarchy with information, and no ground truth. MLflow allows us to do some work with prompt optimization and guidance. This notebook bootstraps that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a69700-e066-4578-a7db-dd9a5e7a60f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow openai --upgrade\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_spark\n",
    "\n",
    "spark = get_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf4256e-0b85-4684-b58a-5a98242acb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM shm.spend.test\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5c8be1-1356-4980-9783-042b0cc2615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LLM_ENDPOINT = \"databricks-claude-sonnet-4\"\n",
    "EXPERIMENT = \"/Workspace/Users/scott.mckean@databricks.com/experiments/spend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb8d48f-076a-4148-a117-de3dc867c56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(EXPERIMENT)     \n",
    "\n",
    "mlflow.openai.autolog()         \n",
    "\n",
    "w = WorkspaceClient()\n",
    "user_name = w.current_user.me().user_name\n",
    "user_id = w.current_user.me().id\n",
    "openai_client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "experiment = w.experiments.get_by_name(EXPERIMENT).experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2048b8f3-9b9e-4abf-8f69-868f7346fbc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_messages(\n",
    "  system_prompt: str, \n",
    "  categories: dict, \n",
    "  invoice_text: str\n",
    "  ):\n",
    "    cats = \"\\n\".join(f\"{k}: {v}\" for k, v in categories.items())\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt + \"\\n\\nCategories:\\n\" + cats,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Invoice:\\n{invoice_text}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "system_prompt = \"You classify invoices into exactly one category and explain why.\"\n",
    "\n",
    "categories = {\n",
    "    \"UTILITIES\": \"Electricity, water, gas, and other recurring utility bills.\",\n",
    "    \"SAAS\": \"Software-as-a-service subscriptions and cloud tools.\",\n",
    "    \"OTHER\": \"Anything that does not fit utilities or SaaS.\",\n",
    "}\n",
    "\n",
    "def classify_invoice(\n",
    "  openai_client, model, invoice_text: str, temperature=0.1):\n",
    "    messages = build_messages(system_prompt, categories, invoice_text)\n",
    "\n",
    "    # This call is automatically traced (inputs, outputs, latency, etc.)\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return messages, resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43bf693-a305-48a7-9ff2-fdce90fe8dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classify_invoice(\n",
    "  openai_client, \n",
    "  LLM_ENDPOINT, \n",
    "  df.sample(1).iloc[0].combined\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fed1c3-a653-4431-840d-d0a3899a4cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow_trace_id =mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=['timestamp DESC'],\n",
    "    max_results=1\n",
    ").iloc[0].trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c04457a-1c53-4fff-be0e-f6a94bc42e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow_trace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c509b3c-088f-4524-9a37-fed5347c1602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import AssessmentSource, AssessmentSourceType\n",
    "\n",
    "mlflow.log_feedback(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"correctness\",\n",
    "    value=4, # correctness score (0: dismal - 10: perfect)\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN, source_id=user_name\n",
    "    ),\n",
    "    )\n",
    "\n",
    "# freeform feedback\n",
    "mlflow.log_feedback(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"guidance\",\n",
    "    value=\"\"\"\n",
    "        Category is correct the rationale is wrong, it should be other because there isn't enough data\n",
    "    \"\"\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN, source_id=user_name\n",
    "    ),\n",
    "    )\n",
    "\n",
    "mlflow.log_expectation(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"cat_lvl_1\",\n",
    "    value=\"OTHER\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=user_name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mlflow.log_expectation(\n",
    "    trace_id=mlflow_trace_id,\n",
    "    name=\"cat_lvl_2\",\n",
    "    value=\"test\",\n",
    "    source=AssessmentSource(\n",
    "        source_type=AssessmentSourceType.HUMAN,\n",
    "        source_id=user_name,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6333049142086926,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "3_optimizer_eval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
